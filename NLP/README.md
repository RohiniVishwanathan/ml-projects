Natural Language Processing Portfolio â€“ Rohini Vishwanathan
This folder contains my natural language processing coursework and applied projects completed as part of a graduate-level NLP class at Claremont Graduate University.
The work spans the full NLP pipelineâ€”preprocessing, feature engineering, vectorization, topic modeling, BERT embedding, and text classificationâ€”applied on real-world datasets such as Yelp reviews.
Each lab includes both:
A Jupyter Notebook (code)
A PDF report (summary + results)
ðŸ”¹ Contents
Lab 1: Text Preprocessing
Cleaning â†’ tokenization â†’ lemmatization â†’ stopword removal
Built a preprocessing pipeline using NLTK and spaCy
Removed punctuation, digits, and noise
Prepared clean token sets for downstream modeling
Files:
Lab1_Rohini_Text_Preprocessing.ipynb
Lab1_Rohini_Text_Preprocessing.pdf
Lab 2: Basic NLP Functions
Utility NLP methods for downstream feature engineering
Implemented normalization, tokenization, POS tagging, stemming
Created reusable helper functions for analysis pipelines
Files:
Lab2_Rohini_NLP_Functions.ipynb
Lab2_Rohini_NLP_Functions.pdf
Lab 3: Feature Vectorization & Sentiment Analysis
BoW â†’ TF-IDF â†’ Word2Vec â†’ Sentiment Models
Built Bag-of-Words and TF-IDF vectorizers
Trained Word2Vec embeddings
Performed sentiment scoring using TextBlob & VADER
Compared vectorization methods for classification tasks
Files:
Lab3_Rohini_Vectorization_Sentiment.ipynb
Lab3_Rohini_Vectorization_Sentiment.pdf
Lab 4: Pre-trained BERT Embeddings
Transformer-based contextual vectorization
Used BERT and DistilBERT models from HuggingFace
Generated sentence-level embeddings
Compared classical vs transformer embeddings
Visualized embedding distribution
Files:
Lab4_Rohini_BERT_Embeddings.ipynb
Lab4_Rohini_BERT_Embeddings.pdf
Lab 5: Text Classification
Traditional ML + dimensionality reduction
Created classification models (Logistic Regression, SVM, Gradient Boosting)
Applied SVD to reduce feature dimensionality
Evaluated accuracy, F1, and performance differences
Files:
Lab5_Rohini_Text_Classification.ipynb
Lab5_Rohini_Text_Classification.pdf
Lab 6: Topic Modeling (LSA, TF-IDF LSA, LDA)
Unsupervised topic extraction on Yelp reviews
Built LSA models with BoW & TF-IDF
Tuned number of topics using coherence scores
Created LDA topic model with Gensim
Visualized topics using pyLDAvis
Labeled and interpreted final topics
Files:
Lab6_Rohini_Topic_Modeling.ipynb
Lab6_Rohini_Topic_Modeling.pdf
Take-Home NLP Exam
Hybrid feature modeling + multi-step NLP pipeline
Combined TF-IDF, LIWC, and metadata features
Modeled personality traits using ML
Conducted interpretation-driven analysis
Delivered final evaluation and model selection strategy
Files:
TakeHome_Rohini_NLP_Exam.ipynb
TakeHome_Rohini_NLP_Exam.pdf
ðŸ”¹ Skills Demonstrated
Text preprocessing (tokenization, lemmatization, normalization)
Feature engineering (TF-IDF, BoW, Word2Vec, FastText, LIWC)
Transformer models (BERT, DistilBERT)
Topic modeling (LSA, LDA, coherence evaluation)
Sentiment analysis
ML classification for NLP
Data visualization (pyLDAvis, matplotlib, seaborn)
Reproducible NLP workflows
ðŸ”¹ Purpose of This Portfolio
To demonstrate practical, end-to-end NLP capability using both classical and transformer-based methods, with a strong emphasis on model evaluation, interpretability, and clean analytical workflow.
